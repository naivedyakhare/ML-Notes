{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b39dbb-d5f1-4928-a1ea-7c3b6b70a033",
   "metadata": {},
   "source": [
    "# Important Points\n",
    "- Overfitting (Low Bias, High Variance): Happens when model fits the training data but gives error in testing data.\n",
    "- Underfitting (High Bias, High Variance): Happens when model doesn't fit training or testing data.\n",
    "- Ridge and LASSO regression works by introducing a hyperparameter Lambda which adds to cost function.\n",
    "    - In Ridge and LASSO, an additional lambda term is added to the cost function that reduces overfitting and can also help in feature selection.\n",
    "- Gradient Descent works by going downwards towards the global minima. The length of the step depends on the learning rate (alpha) which is a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cededa9f-e0fe-4de2-814f-613fa9236275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5642f103-99a5-43bb-b7a0-890d8f0998e9",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Assumptions\n",
    "- Linearity\n",
    "- Normality - Use Logarithmic, Box-cox, Square-root transformation\n",
    "- Homoscedasticity - Variance of residual is same for every value of X\n",
    "- Independence\n",
    "\n",
    "## Advantages\n",
    "- Good for Linear Separable data\n",
    "- Easy to train\n",
    "- Handle overfitting using Dimentionality reduction, Cross validation and regularization\n",
    "\n",
    "## Disadvantages\n",
    "- MultiColinearity (Can be solved using Ridge Regression, PCA, LASSO)\n",
    "- Prune to noise and overfitting\n",
    "- Lot of feature engineering is required\n",
    "\n",
    "## Additional Points\n",
    "- Feature scaling is required because without it, the gradient descent will take time to reach global minima (Min-max scaling, StandardScaler)\n",
    "- Sensetive to missing data\n",
    "- Sensetive to outliers\n",
    "- R<sup>2</sup> and Adjusted R<sup>2</sup> are metrics used to measure goodness of fit.\n",
    "    - R<sup>2</sup> ranges from 0 to 1 and is simple.\n",
    "    - Adjusted R<sup>2</sup> changes with adding features. If feature added is meaningful, the value increases else it decreases. This is done by penalising the atteibutes with less meaning to the fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7de926-411f-4c0b-9954-55416e223802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cd2f3c-bf20-49c4-abe3-469209ed5cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# DECORATOR\n",
    "\n",
    "def hello(func):  \n",
    "    def wrapper():\n",
    "        func()\n",
    "        print(\"Hello, World!\")\n",
    "    return wrapper\n",
    "    \n",
    "@hello\n",
    "def bye():\n",
    "    print(\"Bye, World!\")\n",
    "\n",
    "bye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3bb33-2644-4a80-8d1b-1f8cd5ca4bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7707155c-87fc-4251-bb09-3a613ef5b2cb",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Some Points\n",
    "- Low Bais, High Variance: Can be fixed by making the tree shallow or by using using XGBoost where multiple shallow trees are created and the dataset are passed on to next DT with the weights of the incorrect predictions from the previous DT increased.\n",
    "- No assumptions in DT\n",
    "- Hyperparameter tuning where we can do post-pruning or pre-prunung of the tree, or set limit to the number of leafs\n",
    "\n",
    "## Advantages\n",
    "- Clear Visualisation, like a if-else clauses\n",
    "- Used for both Classification and regression\n",
    "- No feature scaling required\n",
    "- Handles non-linear parameter easily\n",
    "- Automatically handle missing values\n",
    "- Robust to outliers (Does not get affected by outliers)\n",
    "- Less training time\n",
    "\n",
    "## Disadvantages\n",
    "- Overfitting\n",
    "- High Variance\n",
    "- Unstable: if new datapoint is added, the decision tree is to be recreated\n",
    "- Not suitable for large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff57e-b7b1-435e-806f-047977e9cfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de1c783-cd9f-49ba-8c9c-eaae7d1933d5",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Assumptions\n",
    "- Linear relation between independent features and log odds\n",
    "\n",
    "## Some points\n",
    "- The classification is done in either 1 or -1 (instead of 0, this is because classification becomes easier in that way)\n",
    "- If the multiplication of Y (1 or -1) and the value of distance between the point and the best fit line is +ve, then the point is correctly classified and vice versa.\n",
    "- COST FUNCTION: max(Summation(func(Y<sub>i</sub> * W<sup>t</sup> * x<sub>i</sub>))), &nbsp;&nbsp;&nbsp; where W<sup>t</sup> is basically the slope of the best fit line. \n",
    "- the ***func*** in the above equation is sigmoid function to remove problems due to outliers. (1 / (1 + e<sup>-x</sup>)) where x is the value after calculating Y * W * x.\n",
    "- We find the max because the correctly predicted outputs when put in the above formula gives +ve answer. So summation of all correct predictions will be a large +ve number.\n",
    "- We can use AUC-ROC curve to find the threshold that needs to be set above or below which the input gets classified into either one of the classes.\n",
    "- Feature scaling is required.\n",
    "- Sensetive to missing values\n",
    "- Highly impactful to outliers but can be avoided using sigmoid function.\n",
    "- \n",
    "\n",
    "## Advantages\n",
    "- Less prone to overfitting and can be avoided using L1 and L2 Regularisation techniques\n",
    "\n",
    "## Disadvantages\n",
    "- Feature engineering is required a lot\n",
    "- Multicolinearity causes problems\n",
    "- Prone to noise and overfitting\n",
    "- If not linearly related to log odds, this might problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4a5f8-85f3-4e53-be79-373ed098f05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b871494-8b27-4c34-9ffa-ae186f8a4e02",
   "metadata": {},
   "source": [
    "# Random Forest Classifier (Bagging / Bootstrap Aggregator)\n",
    "## Some points\n",
    "- It is an ensemble technique.\n",
    "- Decision tree has low bias and high variance which is because the tree is constructed to its complete depth causing overfitting.\n",
    "- To overcome this problem, we do bootsrap aggregrating, where we take multiple decision trees in parallel, do row sampling and <br> column sampling and we can even prune the decision trees we created to reduce it's depth all of this to reduce overfitting.\n",
    "- Biased to features having many category.\n",
    "- No feature scaling required\n",
    "- Robust to Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357dfb9-0bcf-4f8a-a426-c7bee584cfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "349026b7-5369-41c0-ac9a-0b602fb289ad",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "- The support vector machine is like Regression but it contains hyperplanes that run parallel to the original line (that separates the positive and negative points).\n",
    "- The two Hyperplanes pass through the points that are nearest to the original line and the slope of these three lines should be in a way such that the marginal distance between the two hyperplanes is maximum.\n",
    "- The two points through which the hyperplane pass are called support vectors.\n",
    "- There is something called SVM-Kernel that can help us in creating higher dimension lines that can handle non-linear separable data.\n",
    "- Feature Scaling is required.\n",
    "- Sensetive to outliers.\n",
    "- SVM can be affected by missing values.\n",
    "- Soft margin means hyperplanes that allow some points to be inside it to reduce overfitting.\n",
    "\n",
    "## Advantages\n",
    "- SVM works well with high dimension features due to kernels (rbf, poly, linear)\n",
    "- Memory efficient\n",
    "- Good when we have no idea of data\n",
    "- less overfitting\n",
    "- works well with semi-structured data like text, images, trees\n",
    "\n",
    "## Disadvantages\n",
    "- More time for large dataset\n",
    "- Hard to choose good kernel\n",
    "- Hard to hyperparameter tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17687c08-51a8-4287-bbff-c0a996e1db77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d676057-42cb-4daa-bb3c-94e9d23e87e5",
   "metadata": {},
   "source": [
    "# Weight initialisation techniques\n",
    "- Uniform Weight initialisation\n",
    "- Xavier Glorot\n",
    "- HE Initialisation\n",
    "\n",
    "# Loss Functions for classification\n",
    "## Binary Class Entropy\n",
    "- Used when facing binary classification problem\n",
    "- −(y⋅log(p)+(1−y)⋅log(1−p))\n",
    "## Categorical class entropy\n",
    "- Used when facing multi-class classification problem\n",
    "- -Sum(y<sub>i</sub> * log(p<sub>i</sub>)) &nbsp; where y<sub>i</sub> is one-hot-encoded vector with 1 for true class and 0 for other.\n",
    "## Sparse Categorical class entropy\n",
    "- Similar to categorical class entropy except the target labels are given as integers instead of one-hot-encoded vector.\n",
    "\n",
    "# Loss Functions for Regression\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "- Root mean squared error\n",
    "\n",
    "# Optimizers\n",
    "## Some Points\n",
    "- **Gradient Descent**\n",
    "    - GD is an optimizer that is used to find the optimal value of a specific parameter on each epoch of a ML Algorithm\n",
    "    - Required huge resources for computation (computational extensive)\n",
    "    - very slow\n",
    "- **Stohcastic Gradient Descent**\n",
    "    - Mini batch SGD contains noise that makes the gradient curve not so smooth. To avoid this, we use SGD with momentum.\n",
    "    - *SGD with Momentum* is basically SGD but it also incorporates previous loss function values to know where it is going in the gradient descent curve.\n",
    "- Adagrad\n",
    "- Adadelta & RMSProp\n",
    "- **ADAM** is the best optimizer that has both (Momentum and AdaGrad)\n",
    "\n",
    "\n",
    "# Vanishing Gradient Problem\n",
    "- This occurs when we are using Sigmoid function\n",
    "- What happens is when we are backpropogating, we calculate the deriviative of sigmoid and the deriviative of the sigmoid function ranges from 0 to 0.25\n",
    "- So when we apply chain rule to backpropogate, the gradient that updates the weights while backpropogating becomes so small that the change in the weights is almost negligible and hence the neural network is like forever stuck.\n",
    "\n",
    "# Exploding Gradient Problem\n",
    "- This occurs in the same way as Vanishing gradient problem except this occurs when the weights are high at the beginning due to which the weight being updated varies a lot and gradient will not converge.\n",
    "- When we use ReLU activation function, HE initialisation technique can be used to avoid Exploding and Vanishing Gradient Problem\n",
    "- When we use Sigmoid, we can use Xavier Glorot weight initialisation technique to avoid Vanishing and Exploding Gradient Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908960a-7924-4d89-8d96-f090e4a64ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f02fe0-4134-41b3-8aa3-1f90614f2f0f",
   "metadata": {},
   "source": [
    "# Minkowski Distance\n",
    "<img src=\"https://rittikghosh.com/images/min.png\">\n",
    "\n",
    "- Minkowski Distance is a generalised form of Manhattan and Euclidean distance where the P represents Euclidean or Manhattan\n",
    "    - P = 1 represents Manhattan distance\n",
    "    - P = 2 represents Euclidean distance\n",
    "    - L1 Norms is called Manhattan Distance\n",
    "    - L2 Norms is Euclidean Distance\n",
    "    \n",
    "# Cosine Similarity\n",
    "- Used in recommendation systems\n",
    "- Cosine Distance = 1 - Cosine Similarity\n",
    "- Cosine Similarity is the angle between the two vectors of whose similiarity we are finding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e500-f761-4e20-bdda-8b3e8ea588cf",
   "metadata": {},
   "source": [
    "# Miscellaneous Points\n",
    "- Gradient.ai is a website that proives AI solution and their API can be used to create fine-tuned model using our own data.\n",
    "- Gradient is the only platform that enables healthcare organizations to combine industry expert LLMs like Nightingale, with their own institutional knowledge and data.\n",
    "- Nightingale Open Science provides open data for public use from multiple companies in ethical way.\n",
    "- With data privacy a significant concern, HIPAA (Health Insurance Portability and Accountability Act) and SOC2 (System and Organizations Controls) are federal standards for protecting and securing PHI.\n",
    "- Word Embedding is done using gensim library\n",
    "    - 300 dimensions are selected where each word made into vector of 300 dimensions.\n",
    "    - Each value in those 300 dimensions represent how closely the word is related to each of those features.\n",
    "- Sentence Embedding is done using Doc2Vec library\n",
    "    - Each sentence in itself converted into a vector without breaking into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65d734-720e-4009-84dc-6bc79312eaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7a18eac-b08b-4986-ad4f-37201ae975a2",
   "metadata": {},
   "source": [
    "# How we decide how many number of neurons and layers we need in our deep learning model?\n",
    "- Kerastuner is a library that can be used for where we can handle hyperparameters like the range of layers or number of neurons in each layer and kerastuner will give us the most optimized number of layer and neurons for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50069811-0924-4867-9abe-1aca7337c0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c040b6-27ab-4ed0-92f4-5cdcdacc5896",
   "metadata": {},
   "source": [
    "# Large Language Model\n",
    "### Basically Large language model is a general term and not an architecture. Large Langauge models usually work on the architecture of Transformers.\n",
    "## ChatGPT\n",
    "- When we give audio input to ChatGPT, it first converts the audio to text using Whisper.\n",
    "- Then uses GPT (Generative Pre-trained Transformer) to encode the text and analyse it.\n",
    "- Then uses DALL-E to convert the encoded text to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8f687-b8dd-4006-9a0d-a690f28e21ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64151180-5310-46b8-a275-4f2469c73924",
   "metadata": {},
   "source": [
    "# Reinforment Learning\n",
    "- Agent and Environment interact with each other and that basis the environment rewards(+ve or -ve) the agent for every correct step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5446bed9-bf00-4102-bf89-c4c5f49bd33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(func(\u001b[38;5;241m40\u001b[39m))\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "    \u001b[1;31m[... skipping similar frames: func at line 4 (21 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m func(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(n)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(n):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def func(n):\n",
    "    if n == 1 or n == 0 or n == 2:\n",
    "        return 1\n",
    "    return func(n-1) + func(n-2)\n",
    "print(func(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb732a-2b6d-4ecd-96e0-9d07b61b890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 1 2 3 5 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff9fc2-2f32-4689-abe6-a7246f99de81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
